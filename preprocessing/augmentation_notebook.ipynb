{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Dennis Adelved.. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import bruges\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "from scipy import signal\n",
    "import matplotlib.patches as patch\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import inspect\n",
    "import sys\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Perform the convolution along the height of the image, given a wavelet\n",
    "def convolve_image_from_path(image_path,frequency,axis=0,rgb='True'):\n",
    "    \n",
    "    channels = 0\n",
    "    \n",
    "    wavelet = bruges.filters.ricker(duration=0.100, dt=0.001, f=frequency) #wavelet\n",
    "    \n",
    "    if rgb == True:\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image = np.asarray(image)\n",
    "        channels = image.shape[2]\n",
    "        new_im = image.copy()\n",
    "        \n",
    "        for c in range(new_im.shape[2]):\n",
    "            new_im[:,:,c] = (np.apply_along_axis(lambda t: np.convolve(t,wavelet,mode='same'),\n",
    "                                                 axis=axis,arr=new_im[:,:,c]) )\n",
    "        \n",
    "    elif rgb == False:\n",
    "        image = Image.open(image_path).convert('LA')\n",
    "        image = np.asarray(image)[:,:,0]\n",
    "        new_im = image.copy()\n",
    "        new_im = np.apply_along_axis(lambda t: np.convolve(t,wavelet,mode='same'),axis=axis,arr=new_im)\n",
    "\n",
    "    \n",
    "    return new_im\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to save the relevant attributes(folder,file name and path) of the xml's outputted from LabelImg\n",
    "def change_xml(xml_path,new_folder,new_fname,new_path,channels=3):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    root[0].text = new_folder\n",
    "    root[1].text = new_fname\n",
    "    root[2].text = new_path\n",
    "    if channels==1:\n",
    "        root[4][2].text = '1'\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "    \n",
    "#convolve grayscale image with random frequency in range [min_freq:10:max_freq]    \n",
    "def convolve_gray_image_random(im_path, min_freq,max_freq, multiple_axes=True):\n",
    "    image = Image.open(im_path).convert('L')\n",
    "    image = np.asarray(image)\n",
    "    \n",
    "    f = []\n",
    "    \n",
    "    f.append(random.randint(min_freq,max_freq))\n",
    "    f.append(random.randint(min_freq,max_freq))\n",
    "    \n",
    "    wavelet = []\n",
    "    \n",
    "    wavelet.append(bruges.filters.ricker(duration=0.100, dt=0.001, f=f[0])) #wavelet\n",
    "    wavelet.append(bruges.filters.ricker(duration=0.100, dt=0.001, f=f[1])) \n",
    "    \n",
    "    if multiple_axes == True:\n",
    "        vert = np.apply_along_axis(lambda t: np.convolve(t,wavelet[0],mode='same'),axis=0,arr=image)\n",
    "        hor = np.apply_along_axis(lambda t: np.convolve(t,wavelet[1],mode='same'),axis=1,arr=image)\n",
    "    \n",
    "        return [vert,hor]\n",
    "    else:\n",
    "        hor = np.apply_along_axis(lambda t: np.convolve(t,wavelet[0],mode='same'),axis=1,arr=image)\n",
    "        return [hor]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "#augmentation gray\n",
    "def convert_to_gray(image_path):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = np.asarray(image)\n",
    "    new_im = image.copy()\n",
    "    return new_im\n",
    "#augmentation sobel    \n",
    "def augment_sobel(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    im = np.asarray(image)\n",
    "    sobelx = cv2.Sobel(im,cv2.CV_64F,1,0,ksize=3)\n",
    "    sobely = cv2.Sobel(im,cv2.CV_64F,0,1,ksize=3)\n",
    "    grad = np.sqrt(sobelx**2 + sobely**2)\n",
    "    return grad\n",
    "    \n",
    "#augmentation canny    \n",
    "def augment_canny(image_path):\n",
    "    percentiles = np.array([80, 85, 90, 95])\n",
    "    image = Image.open(image_path)\n",
    "    im = np.asarray(image)\n",
    "    perc = percentiles[random.randrange(len(percentiles))]\n",
    "    canny = cv2.Canny(im,perc/2,perc)\n",
    "    return canny\n",
    "    \n",
    "\n",
    "#import xml and jpg and returns a list of xml,jpg pairs\n",
    "def import_img_xml_paths(data_dir):\n",
    "    used_cores = glob.glob(os.path.join(data_dir,'*.jpg'))\n",
    "    used_xml = glob.glob(os.path.join(data_dir,'*.xml'))\n",
    "    return list(zip(sorted(used_cores),sorted(used_xml)))\n",
    "    \n",
    "    \n",
    "#splits training and validation data, simple hold-out split    \n",
    "def simple_holdout_split(image_xml_pairs,split=0.2):\n",
    "    \n",
    "    num_im = len(image_xml_pairs)\n",
    "    \n",
    "    random.shuffle(image_xml_pairs)\n",
    "    val = image_xml_pairs[0:math.floor(num_im*split)]\n",
    "    train = image_xml_pairs[math.floor(num_im*split):]\n",
    "    \n",
    "    return train,val\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation script for convolving with random frequency\n",
    "INPUT_DATA = 'folder_name'\n",
    "OUTPUT_DIR = 'output_directory'\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),INPUT_DATA)\n",
    "\n",
    "\n",
    "def convolve_image_data(data_dir,OUTPUT_DIR):\n",
    "    \n",
    "    out  = os.path.join(os.getcwd(), OUTPUT_DIR)\n",
    "    if OUTPUT_DIR not in os.listdir(cwd):\n",
    "        os.makedirs(os.path.join(cwd,OUTPUT_DIR))\n",
    "    \n",
    "    used_cores = glob.glob(os.path.join(data_dir,'*.jpg'))\n",
    "    used_xml = glob.glob(os.path.join(data_dir,'*.xml'))\n",
    "\n",
    "    used_cores = sorted(used_cores)\n",
    "    used_xml = sorted(used_xml)\n",
    "    paths = zip(used_cores,used_xml)\n",
    "\n",
    "\n",
    "    for tup in list(paths):\n",
    "\n",
    "\n",
    "        im_path = tup[0]\n",
    "        xml_path = tup[1]\n",
    "\n",
    "        #gray_im = convert_to_gray(im_path)\n",
    "\n",
    "        aug = convolve_gray_image_random(im_path, 30,100, multiple_axes=True)\n",
    "\n",
    "\n",
    "        vert_aug = aug[0]\n",
    "        hor_aug = aug[1]\n",
    "\n",
    "\n",
    "        vert_name = 'wavelet_vert_' + tup[0].split('/')[-1]\n",
    "        hor_name = 'wavelet_hor_' + tup[0].split('/')[-1]\n",
    "\n",
    "\n",
    "        vert_path = os.path.join(out,vert_name)\n",
    "        hor_path = os.path.join(out,hor_name)\n",
    "\n",
    "\n",
    "        vert_tree = change_xml(xml_path,out.split('/')[-1],vert_name,vert_path,1)\n",
    "        hor_tree = change_xml(xml_path,out.split('/')[-1],hor_name,hor_path,1)\n",
    "\n",
    "\n",
    "        vert_new_xml_path = vert_path.split('.')[0] + '.xml'\n",
    "        hor_new_xml_path = hor_path.split('.')[0] + '.xml'\n",
    "\n",
    "\n",
    "        vert_im = Image.fromarray(vert_aug)\n",
    "        hor_im = Image.fromarray(hor_aug)\n",
    "\n",
    "\n",
    "        plt.imsave(vert_path,vert_im,cmap='gray')\n",
    "        plt.imsave(hor_path,hor_im,cmap='gray')\n",
    "\n",
    "        vert_tree.write(vert_new_xml_path)\n",
    "        hor_tree.write(hor_new_xml_path)\n",
    "\n",
    "convolve_image_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define paths for input and output\n",
    "cwd = os.getcwd()\n",
    "#Path to directory containing the images with their respective xml files\n",
    "SRC_TRAIN = os.path.join(cwd,'training_directory')\n",
    "SRC_VALID = os.path.join(cwd,'validation_directory') \n",
    "SRC_TEST = os.path.join(cwd,'test_directory') \n",
    "\n",
    "\n",
    "#output directory to save the augmented images and xmls\n",
    "SAVE_DIR_TRAIN = 'save_directory_augmented_training' \n",
    "SAVE_DIR_VALID = 'save_directory_augmented_validation' \n",
    "SAVE_DIR_TEST = 'save_directory_augmented_test'\n",
    "\n",
    "\n",
    "#choose type of augmentation ['gray','sobel','canny']\n",
    "augmentation_type = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = SRC_TEST\n",
    "\n",
    "\n",
    "used_cores = glob.glob(os.path.join(data_dir,'*.jpg'))\n",
    "used_xml = glob.glob(os.path.join(data_dir,'*.xml'))\n",
    "\n",
    "used_cores = sorted(used_cores)\n",
    "used_xml = sorted(used_xml)\n",
    "paths = zip(used_cores,used_xml)\n",
    "\n",
    "if data_dir == SRC_VALID:\n",
    "    aug_dest = os.path.join(cwd,SAVE_DIR_VALID)\n",
    "elif data_dir==SRC_TEST:\n",
    "    aug_dest = os.path.join(cwd,SAVE_DIR_TEST)\n",
    "else:\n",
    "    aug_dest = os.path.join(cwd,SAVE_DIR_TRAIN)\n",
    "\n",
    "\n",
    "for tup in list(paths):\n",
    "    \n",
    "    \n",
    "    im_path = tup[0]\n",
    "    xml_path = tup[1]\n",
    "    \n",
    "    if augmentation_type == 'gray':\n",
    "        augmented = convert_to_gray(im_path)\n",
    "    elif augmentation_type == 'canny':\n",
    "        augmented = augment_canny(im_path)\n",
    "    elif augmentation_type == 'sobel':\n",
    "        augmented = augment_sobel(im_path)\n",
    "    else:\n",
    "        print('choose a valid augmentation type')\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    new_name = tup[0].split('/')[-1]\n",
    "\n",
    "    new_path = os.path.join(aug_dest,new_name)\n",
    "    print(new_path)\n",
    "\n",
    "    tree = change_xml(xml_path,aug_dest.split('/')[-1],new_name,new_path,1)\n",
    "\n",
    "    new_xml_path = new_path.split('.')[0] + '.xml'\n",
    "\n",
    "    print(new_name)\n",
    "    \n",
    "    plt.imsave(new_path,augmented,cmap='gray')\n",
    "    \n",
    "    tree.write(new_xml_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold validation split\n",
    "#splitting the easy test set into 5 folds (training,validaiton and test set)\n",
    "\n",
    "#define number of folds\n",
    "samples = ['fold1','fold2','fold3','fold4','fold5']\n",
    "\n",
    "data_dir = 'test_data' #data directory containing the xml and jpg files\n",
    "\n",
    "samps = []\n",
    "for s in samples:\n",
    "    splits=[]\n",
    "    splits.append('train_'+s)\n",
    "    splits.append('valid_'+s)\n",
    "    splits.append('test_'+s)\n",
    "    samps.append(splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for s in samps:\n",
    "    xmls = glob.glob(os.path.join(data_dir,'*.xml'))\n",
    "    ims = glob.glob(os.path.join(data_dir,'*.jpg'))\n",
    "    xmls=sorted(xmls)\n",
    "    ims = sorted(ims)\n",
    "    data = zip(ims,xmls)\n",
    "    data = list(data)\n",
    "    random.shuffle(data)\n",
    "    split = data[0:15]\n",
    "    test = data[15:]\n",
    "    random.shuffle(split)\n",
    "    train=split[0:10]\n",
    "    val = split[10:]\n",
    "    for f in s:\n",
    "        os.mkdir(f)\n",
    "        if 'train' in f:\n",
    "            for i in range(len(train)):\n",
    "                imin = os.path.join(os.getcwd(),train[i][0])\n",
    "                xmlin = os.path.join(os.getcwd(),train[i][1])\n",
    "                imout = os.path.join(os.getcwd(),f,train[i][0].split('/')[-1])\n",
    "                xmlout = os.path.join(os.getcwd(),f,train[i][1].split('/')[-1])\n",
    "                shutil.copy(imin,imout)\n",
    "                shutil.copy(xmlin,xmlout)\n",
    "        if 'valid' in f:\n",
    "            for i in range(len(val)):\n",
    "                imin = os.path.join(os.getcwd(),val[i][0])\n",
    "                xmlin = os.path.join(os.getcwd(),val[i][1])\n",
    "                imout = os.path.join(os.getcwd(),f,val[i][0].split('/')[-1])\n",
    "                xmlout = os.path.join(os.getcwd(),f,val[i][1].split('/')[-1])\n",
    "                shutil.copy(imin,imout)\n",
    "                shutil.copy(xmlin,xmlout)\n",
    "        if 'test' in f:\n",
    "            for i in range(len(test)):\n",
    "                imin = os.path.join(os.getcwd(),test[i][0])\n",
    "                xmlin = os.path.join(os.getcwd(),test[i][1])\n",
    "                imout = os.path.join(os.getcwd(),f,test[i][0].split('/')[-1])\n",
    "                xmlout = os.path.join(os.getcwd(),f,test[i][1].split('/')[-1])\n",
    "                shutil.copy(imin,imout)\n",
    "                shutil.copy(xmlin,xmlout)    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
